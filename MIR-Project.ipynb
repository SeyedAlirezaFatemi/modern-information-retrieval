{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import multiprocessing\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "import untangle\n",
    "\n",
    "from hazm import *\n",
    "from typing import List, NewType\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.setrecursionlimit(10 ** 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = Normalizer()\n",
    "word_tokenizer = WordTokenizer(separate_emoji=True)\n",
    "stemmer = Stemmer()\n",
    "\n",
    "\n",
    "def normalize_text(raw_text: str) -> str:\n",
    "    return normalizer.normalize(raw_text)\n",
    "\n",
    "\n",
    "def tokenize_text(normalized_text: str) -> List[str]:\n",
    "    return word_tokenizer.tokenize(normalized_text)\n",
    "\n",
    "\n",
    "def remove_punctuation(tokens: List[str]) -> List[str]:\n",
    "    pattern = re.compile(r\"[.!?;\\\\-]\")\n",
    "    return list(filter(lambda token: not re.match(pattern, token), tokens))\n",
    "\n",
    "\n",
    "def stem_tokens(tokens: List[str]) -> List[str]:\n",
    "    return list(map(lambda token: stemmer.stem(token), tokens))\n",
    "\n",
    "\n",
    "def lemmatize_tokens(tokens: List[str]) -> List[str]:\n",
    "    lemmatizer = Lemmatizer()\n",
    "    return list(map(lambda token: lemmatizer.lemmatize(token), tokens))\n",
    "\n",
    "\n",
    "def prepare_text(\n",
    "    raw_text: str,\n",
    "    stem: bool = True,\n",
    "    del_punctuation: bool = False,\n",
    "    lemmatize: bool = False,\n",
    "    debug: bool = False,\n",
    ") -> List[str]:\n",
    "    normalized_text = normalize_text(raw_text)\n",
    "    if debug:\n",
    "        print(normalized_text)\n",
    "    tokens = tokenize_text(normalized_text)\n",
    "    if del_punctuation:\n",
    "        tokens = remove_punctuation(tokens)\n",
    "    if stem:\n",
    "        tokens = stem_tokens(tokens)\n",
    "    if lemmatize:\n",
    "        tokens = lemmatize_tokens(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DocId = NewType(\"DocId\", int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Document:\n",
    "    title_tokens: List[str] = []\n",
    "    text_tokens: List[str] = []\n",
    "\n",
    "    def __init__(self, doc_id: DocId, title: str, text: str):\n",
    "        self.doc_id = doc_id\n",
    "        self.title = title\n",
    "        self.text = text\n",
    "        self.title_tokens = prepare_text(title)\n",
    "        self.text_tokens = prepare_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingListItem:\n",
    "    def __init__(self, doc_id: DocId):\n",
    "        self.doc_id = doc_id\n",
    "        self.title_positions = []\n",
    "        self.text_positions = []\n",
    "\n",
    "    def add_to_positions(self, field: str, position: int):\n",
    "        self.__getattribute__(f\"{field}_positions\").append(position)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"\"\"\n",
    "        Document ID: {self.doc_id}\n",
    "        Title Positions: {self.title_positions}\n",
    "        Text Positions: {self.text_positions}\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_doc(page, debug: bool = False):\n",
    "    if debug:\n",
    "        print(f\"Document {page.id.cdata} started!\")\n",
    "    doc = Document(\n",
    "        int(page.id.cdata), page.title.cdata, page.revision.text.cdata\n",
    "    )\n",
    "    if debug:\n",
    "        print(f\"Document {page.id.cdata} done!\")\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_documents(\n",
    "    docs_path: str = \"./data/Persian.xml\", multiprocess: bool = False\n",
    ") -> List[Document]:\n",
    "    tree = untangle.parse(docs_path)\n",
    "    documents: List[Document] = []\n",
    "\n",
    "    if multiprocess:\n",
    "        pool = multiprocessing.Pool()\n",
    "        for page in tree.mediawiki.page:\n",
    "            documents.append(pool.apply_async(create_doc, args=(page,)))\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        documents = [res.get() for res in documents]\n",
    "    else:\n",
    "        for page in tqdm(tree.mediawiki.page):\n",
    "            documents.append(\n",
    "                Document(\n",
    "                    int(page.id.cdata),\n",
    "                    page.title.cdata,\n",
    "                    page.revision.text.cdata,\n",
    "                )\n",
    "            )\n",
    "    documents = sorted(documents, key=lambda document: document.doc_id)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_positional_indexes(\n",
    "    docs_path: str = \"./data/Persian.xml\", multiprocess: bool = False\n",
    "):\n",
    "    documents = create_documents(docs_path, multiprocess)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1572/1572 [00:15<00:00, 99.66it/s] \n"
     ]
    }
   ],
   "source": [
    "documents = construct_positional_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"documents.pickle\", \"wb\") as f:\n",
    "    pickle.dump(documents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1572/1572 [00:08<00:00, 186.23it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_index = dict()\n",
    "for document in tqdm(documents):\n",
    "    token_positional_list_item_dict = dict()\n",
    "    for field in [\"title\", \"text\"]:\n",
    "        for idx, token in enumerate(\n",
    "            document.__getattribute__(f\"{field}_tokens\")\n",
    "        ):\n",
    "            if token not in token_positional_list_item_dict:\n",
    "                token_positional_list_item_dict[token] = PostingListItem(\n",
    "                    document.doc_id\n",
    "                )\n",
    "            token_positional_list_item_dict[token].add_to_positions(field, idx)\n",
    "    for token in token_positional_list_item_dict:\n",
    "        if token not in corpus_index:\n",
    "            corpus_index[token] = []\n",
    "        corpus_index[token].append(token_positional_list_item_dict[token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.main import construct_positional_indexes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../MIR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
