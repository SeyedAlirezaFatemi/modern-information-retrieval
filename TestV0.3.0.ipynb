{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docker run -p 9200:9200 -p 9300:9300 -e \"discovery.type=single-node\" docker.elastic.co/elasticsearch/elasticsearch:7.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawl "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "from paper_crawler.paper_crawler.spiders.semanticscholar import (\n",
    "    SemanticscholarSpider,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess(\n",
    "    settings={\n",
    "        \"FEEDS\": {\"papers.json\": {\"format\": \"json\"}},\n",
    "        \"LOG_ENABLED\": False\n",
    "        #     \"LOG_LEVEL\": 'INFO',\n",
    "    }\n",
    ")\n",
    "process.crawl(SemanticscholarSpider, max_papers=500)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Crawled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"papers.json\") as f:\n",
    "    items = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'f90720ed12e045ac84beb94c27271d6fb8ad48cf',\n",
       " 'title': 'The Lottery Ticket Hypothesis: Training Pruned Neural Networks',\n",
       " 'abstract': 'Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the \"lottery ticket hypothesis,\" proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these \"lottery ticketsâ€¦\\xa0',\n",
       " 'date': '2018',\n",
       " 'references': ['34f25a8704614163c4095b3ee2fc969b60de4698',\n",
       "  '1ff9a37d766e3a4f39757f5e1b235a42dacf18ff',\n",
       "  'b0bd441a0cc04cdd0d0e469fe4c5184ee148a97d',\n",
       "  'cc46229a7c47f485e090857cbab6e6bf68c09811',\n",
       "  '642d0f49b7826adcf986616f4af77e736229990f',\n",
       "  '049fd80f52c0b1fa4d532945d95a24734b62bdf3',\n",
       "  '2dfef5635c8c44431ca3576081e6cfe6d65d4862',\n",
       "  '397de65a9a815ec39b3704a79341d687205bc80a',\n",
       "  'c2a1cb1612ba21e067a5c3ba478a8d73b796b77a',\n",
       "  'e8eaf8aedb495b6ae0e174eea11e3cfcdf4a3724'],\n",
       " 'authors': ['Jonathan Frankle', 'Michael Carbin']}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "503"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(hosts=[{\"host\": \"localhost\", \"port\": 9200}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear previous index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'error': {'root_cause': [{'type': 'index_not_found_exception',\n",
       "    'reason': 'no such index [paper-index]',\n",
       "    'resource.type': 'index_or_alias',\n",
       "    'resource.id': 'paper-index',\n",
       "    'index_uuid': '_na_',\n",
       "    'index': 'paper-index'}],\n",
       "  'type': 'index_not_found_exception',\n",
       "  'reason': 'no such index [paper-index]',\n",
       "  'resource.type': 'index_or_alias',\n",
       "  'resource.id': 'paper-index',\n",
       "  'index_uuid': '_na_',\n",
       "  'index': 'paper-index'},\n",
       " 'status': 404}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(\"paper-index\", ignore=404)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'paper-index'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index=\"paper-index\", ignore=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create persistent layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import (\n",
    "    Document,\n",
    "    Date,\n",
    "    Nested,\n",
    "    Boolean,\n",
    "    analyzer,\n",
    "    InnerDoc,\n",
    "    Completion,\n",
    "    Keyword,\n",
    "    Text,\n",
    "    Integer,\n",
    "    Float,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper(Document):\n",
    "    title = Text(fields={\"raw\": Keyword()})\n",
    "    date = Integer()\n",
    "    abstract = Text()\n",
    "    authors = Text()\n",
    "    references = Text()\n",
    "    page_rank = Float()\n",
    "\n",
    "    class Index:\n",
    "        name = \"paper-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the mappings in Elasticsearch\n",
    "Paper.init(using=es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import TransportError\n",
    "from elasticsearch.helpers import bulk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503, [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://elasticsearch-py.readthedocs.io/en/master/helpers.html#bulk-helpers\n",
    "# https://www.elastic.co/guide/en/elasticsearch/reference/master/docs-bulk.html\n",
    "\n",
    "\n",
    "def gendata():\n",
    "    for idx, item in enumerate(items):\n",
    "        if item[\"date\"] == \"\" or not item[\"date\"].isdigit():\n",
    "            del item[\"date\"]\n",
    "        yield {\n",
    "            \"_index\": \"paper-index\",\n",
    "            \"_id\": item[\"id\"],\n",
    "            \"page_rank\": 1.0,\n",
    "            **item,\n",
    "        }\n",
    "\n",
    "\n",
    "bulk(es, gendata())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in items:\n",
    "    paper = Paper(meta={\"id\": item[\"id\"]}, page_rank=1.0, **item)\n",
    "    paper.save(using=es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcions for inserting items and clearing index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_utils(host=None):\n",
    "    if host is None:\n",
    "        host = {\"host\": \"localhost\", \"port\": 9200}\n",
    "    es = Elasticsearch(hosts=[{\"host\": \"localhost\", \"port\": 9200}])\n",
    "\n",
    "    def clear_index():\n",
    "        es.indices.delete(\"paper-index\", ignore=404)\n",
    "        es.indices.create(index=\"paper-index\", ignore=400)\n",
    "\n",
    "    def insert_items(items):\n",
    "        for item in items:\n",
    "            paper = Paper(meta={\"id\": item[\"id\"]}, **item)\n",
    "            paper.save(using=es)\n",
    "\n",
    "    return clear_index, insert_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear, insert = gen_utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = list(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = sorted(list(map(lambda x: x[\"id\"], all_papers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_matrix = np.zeros((len(all_ids), len(all_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_loc = dict()\n",
    "for index, paper_id in enumerate(all_ids):\n",
    "    id_loc[paper_id] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, paper_id in enumerate(all_ids):\n",
    "    paper = Paper.get(id=paper_id, using=es)\n",
    "    if paper.references is not None:\n",
    "        for reference_id in paper.references:\n",
    "            try:\n",
    "                p_matrix[index, id_loc[reference_id]] = 1\n",
    "            except KeyError:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "N = len(all_ids)\n",
    "v = np.ones((1, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = np.sum(p_matrix, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first part is for rows having nonzero elements\n",
    "# second part is for dead-ends\n",
    "p_matrix = ((row_sums > 0) * 1) * (\n",
    "    (1 - alpha) * p_matrix / (row_sums + np.logical_not(row_sums > 0) * 1)\n",
    "    + alpha * v / N\n",
    ") + (np.logical_not(row_sums > 0) * 1) * v / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.ones((1, N)) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    next_state = x0 @ p_matrix\n",
    "    if np.allclose(next_state, x0, rtol=0.0001):\n",
    "        break\n",
    "    x0 = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, paper_id in enumerate(all_ids):\n",
    "    paper = Paper.get(id=paper_id, using=es)\n",
    "    paper.update(page_rank=next_state[0, index], using=es)\n",
    "    paper.save(using=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution #2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(503, [])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gendata():\n",
    "    for index, paper_id in enumerate(all_ids):\n",
    "        yield {\n",
    "            \"_index\": \"paper-index\",\n",
    "            \"_id\": items[index][\"id\"],\n",
    "            \"_source\": {\"doc\": {\"page_rank\": next_state[0, index]}},\n",
    "            \"_op_type\": \"update\",\n",
    "        }\n",
    "\n",
    "\n",
    "bulk(es, gendata())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 429\n",
    "body = \"\"\n",
    "for index, paper_id in enumerate(all_ids):\n",
    "    body += f\"\"\"{{ \"update\" : {{\"_id\" : \"{paper_id}\", \"_index\" : \"paper-index\"}} }}\n",
    "{{ \"doc\" : {{\"page_rank\" : {next_state[0, index]} }}}}\n",
    "\"\"\"\n",
    "    break\n",
    "body += \"\"\n",
    "res = es.bulk(body)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_search = \"classification\"\n",
    "abstract_search = \"neural\"\n",
    "year_search = 2000\n",
    "title_weight = 50\n",
    "abstract_weight = 40\n",
    "year_weight = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(\n",
    "    title_search: str,\n",
    "    abstract_search: str,\n",
    "    year_search: int,\n",
    "    title_weight: float = 20,\n",
    "    abstract_weight: float = 10,\n",
    "    year_weight: float = 5,\n",
    "    apply_page_rank: bool = True,\n",
    "):\n",
    "    if apply_page_rank:\n",
    "        return es.search(\n",
    "            index=\"paper-index\",\n",
    "            body={\n",
    "                \"query\": {\n",
    "                    # https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-weight\n",
    "                    \"function_score\": {\n",
    "                        \"functions\": [\n",
    "                            {\n",
    "                                \"filter\": {\"match\": {\"title\": title_search}},\n",
    "                                \"weight\": title_weight,\n",
    "                            },\n",
    "                            {\n",
    "                                \"filter\": {\n",
    "                                    \"match\": {\"abstract\": abstract_search}\n",
    "                                },\n",
    "                                \"weight\": abstract_weight,\n",
    "                            },\n",
    "                            # https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-range-query.html\n",
    "                            {\n",
    "                                \"filter\": {\n",
    "                                    \"range\": {\"date\": {\"gte\": year_search}}\n",
    "                                },\n",
    "                                \"weight\": year_weight,\n",
    "                            },\n",
    "                            # https://www.elastic.co/guide/en/elasticsearch/reference/current/static-scoring-signals.html\n",
    "                            # https://www.elastic.co/guide/en/elasticsearch/reference/7.x/query-dsl-rank-feature-query.html#rank-feature-query-saturation\n",
    "                            {\n",
    "                                \"script_score\": {\n",
    "                                    \"script\": {\n",
    "                                        #                                 \"source\": \"_score * saturation(doc['page_rank'].value, 10)\"\n",
    "                                        \"source\": \"_score * doc['page_rank'].value\"\n",
    "                                    }\n",
    "                                }\n",
    "                            },\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "    return es.search(\n",
    "        index=\"paper-index\",\n",
    "        body={\n",
    "            \"query\": {\n",
    "                # https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-weight\n",
    "                \"function_score\": {\n",
    "                    \"functions\": [\n",
    "                        {\n",
    "                            \"filter\": {\"match\": {\"title\": title_search}},\n",
    "                            \"weight\": title_weight,\n",
    "                        },\n",
    "                        {\n",
    "                            \"filter\": {\"match\": {\"abstract\": abstract_search}},\n",
    "                            \"weight\": abstract_weight,\n",
    "                        },\n",
    "                        # https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-range-query.html\n",
    "                        {\n",
    "                            \"filter\": {\n",
    "                                \"range\": {\"date\": {\"gte\": year_search}}\n",
    "                            },\n",
    "                            \"weight\": year_weight,\n",
    "                        },\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large-Margin Classification in Infinite Neural Networks: 5.6436925\n",
      "ImageNet Classification with Deep Convolutional Neural Networks: 2.4965262\n",
      "High-Performance Neural Networks for Visual Object Classification: 1.8588557\n",
      "Training CNNs with Low-Rank Filters for Efficient Image Classification: 0.8912874\n",
      "Character-level Convolutional Networks for Text Classification: 0.7963235\n",
      "Some Improvements on Deep Convolutional Neural Network Based Image Classification: 0.782767\n",
      "Convolutional Neural Networks for Sentence Classification: 0.7670854\n",
      "SVM-KNN: Discriminative Nearest Neighbor Classification for Visual Category Recognition: 0.7627265\n",
      "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification: 0.75370294\n",
      "Visualizing and Understanding Neural Models in NLP: 0.52961636\n"
     ]
    }
   ],
   "source": [
    "res = search(title_search, abstract_search, year_search, apply_page_rank=True)\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    print(f'{hit[\"_source\"][\"title\"]}: {hit[\"_score\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CNNs with Low-Rank Filters for Efficient Image Classification: 1000.0\n",
      "Character-level Convolutional Networks for Text Classification: 1000.0\n",
      "Large-Margin Classification in Infinite Neural Networks: 1000.0\n",
      "Convolutional Neural Networks for Sentence Classification: 1000.0\n",
      "Some Improvements on Deep Convolutional Neural Network Based Image Classification: 1000.0\n",
      "Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification: 1000.0\n",
      "High-Performance Neural Networks for Visual Object Classification: 1000.0\n",
      "ImageNet Classification with Deep Convolutional Neural Networks: 1000.0\n",
      "Approximation algorithms for classification problems with pairwise relationships: metric labeling and Markov random fields: 100.0\n",
      "Part-based statistical models for object classification and detection: 100.0\n"
     ]
    }
   ],
   "source": [
    "res = search(title_search, abstract_search, year_search, apply_page_rank=False)\n",
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    print(f'{hit[\"_source\"][\"title\"]}: {hit[\"_score\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With page rank\n",
    "res = es.search(\n",
    "    index=\"paper-index\",\n",
    "    body={\n",
    "        \"query\": {\n",
    "            # https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-weight\n",
    "            \"function_score\": {\n",
    "                \"functions\": [\n",
    "                    {\n",
    "                        \"filter\": {\"match\": {\"title\": title_search}},\n",
    "                        \"weight\": title_weight,\n",
    "                    },\n",
    "                    {\n",
    "                        \"filter\": {\"match\": {\"abstract\": abstract_search}},\n",
    "                        \"weight\": abstract_weight,\n",
    "                    },\n",
    "                    # https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-range-query.html\n",
    "                    {\n",
    "                        \"filter\": {\"range\": {\"date\": {\"gte\": year_search}}},\n",
    "                        \"weight\": year_weight,\n",
    "                    },\n",
    "                    # https://www.elastic.co/guide/en/elasticsearch/reference/current/static-scoring-signals.html\n",
    "                    # https://www.elastic.co/guide/en/elasticsearch/reference/7.x/query-dsl-rank-feature-query.html#rank-feature-query-saturation\n",
    "                    {\n",
    "                        \"script_score\": {\n",
    "                            \"script\": {\n",
    "                                #                                 \"source\": \"_score * saturation(doc['page_rank'].value, 10)\"\n",
    "                                \"source\": \"_score * doc['page_rank'].value\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Referring Expression Recognition: What Do Systems Actually Learn?: 0.29866457\n",
      "Memory Architectures in Recurrent Neural Network Language Models: 0.27257547\n",
      "Towards a Unified Natural Language Inference Framework to Evaluate Sentence Representations: 0.19155078\n",
      "Dissecting Contextual Word Embeddings: Architecture and Representation: 0.15634124\n",
      "The Lottery Ticket Hypothesis: Training Pruned Neural Networks: 0.13396016\n",
      "Natural Language Inference over Interaction Space: 0.12812993\n",
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: 0.10748777\n",
      "Attention-Based Convolutional Neural Network for Machine Comprehension: 0.097398795\n",
      "A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference: 0.09511792\n",
      "Annotation Artifacts in Natural Language Inference Data: 0.09371895\n"
     ]
    }
   ],
   "source": [
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    print(f'{hit[\"_source\"][\"title\"]}: {hit[\"_score\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without page rank\n",
    "res = es.search(\n",
    "    index=\"paper-index\",\n",
    "    body={\n",
    "        \"query\": {\n",
    "            # https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-weight\n",
    "            \"function_score\": {\n",
    "                \"functions\": [\n",
    "                    {\n",
    "                        \"filter\": {\"match\": {\"title\": title_search}},\n",
    "                        \"weight\": title_weight,\n",
    "                    },\n",
    "                    {\n",
    "                        \"filter\": {\"match\": {\"abstract\": abstract_search}},\n",
    "                        \"weight\": abstract_weight,\n",
    "                    },\n",
    "                    # https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-range-query.html\n",
    "                    {\n",
    "                        \"filter\": {\"range\": {\"date\": {\"gte\": year_search}}},\n",
    "                        \"weight\": year_weight,\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Lottery Ticket Hypothesis: Training Pruned Neural Networks: 100.0\n",
      "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding: 50.0\n",
      "Character-Level Language Modeling with Deeper Self-Attention: 50.0\n",
      "U-Net: Machine Reading Comprehension with Unanswerable Questions: 50.0\n",
      "GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding: 50.0\n",
      "Memory Architectures in Recurrent Neural Network Language Models: 50.0\n",
      "Annotation Artifacts in Natural Language Inference Data: 50.0\n",
      "Transforming Question Answering Datasets Into Natural Language Inference Datasets: 50.0\n",
      "Visual Referring Expression Recognition: What Do Systems Actually Learn?: 50.0\n",
      "Visual Dialog: 50.0\n"
     ]
    }
   ],
   "source": [
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    print(f'{hit[\"_source\"][\"title\"]}: {hit[\"_score\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Search(using=es, index=\"paper-index\").query(\"match\", title=\"the lottery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Search(using=es, index=\"paper-index\").query(\"match\", id=\"323694313\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1711254 The Lottery Ticket Hypothesis: Training Pruned Neural Networks\n",
      "2.6979883 The dropout learning algorithm\n",
      "2.5801163 Return of the Devil in the Details: Delving Deep into Convolutional Nets\n",
      "2.3857658 Exploring the Limits of Language Modeling\n",
      "2.2552712 Distilling the Knowledge in a Neural Network\n",
      "2.2552712 Rethinking the Inception Architecture for Computer Vision\n",
      "2.2552712 Compressing Neural Networks with the Hashing Trick\n",
      "2.1383119 MaskGAN: Better Text Generation via Filling in the ______\n",
      "2.1383119 Improving the speed of neural networks on CPUs\n",
      "2.032885 Addressing the Rare Word Problem in Neural Machine Translation\n"
     ]
    }
   ],
   "source": [
    "for hit in response:\n",
    "    print(hit.meta.score, hit.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by calling .search we get back a standard Search object\n",
    "s = Paper.search(using=es)\n",
    "# the search is already limited to the index and doc_type of our document\n",
    "s = s.query(\"match\", title=\"the lottery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.1711254 The Lottery Ticket Hypothesis: Training Pruned Neural Networks 0.0016542321229066994\n",
      "2.6979883 The dropout learning algorithm 0.0019053149224319082\n",
      "2.5801163 Return of the Devil in the Details: Delving Deep into Convolutional Nets 0.005560533383657462\n",
      "2.3857658 Exploring the Limits of Language Modeling 0.002667048414064376\n",
      "2.2552712 Distilling the Knowledge in a Neural Network 0.00414401990427915\n",
      "2.2552712 Rethinking the Inception Architecture for Computer Vision 0.001837508565113488\n",
      "2.2552712 Compressing Neural Networks with the Hashing Trick 0.0023534950296675962\n",
      "2.1383119 MaskGAN: Better Text Generation via Filling in the ______ 0.0018031145099593263\n",
      "2.1383119 Improving the speed of neural networks on CPUs 0.010104847145196385\n",
      "2.032885 Addressing the Rare Word Problem in Neural Machine Translation 0.004836191907258115\n"
     ]
    }
   ],
   "source": [
    "for hit in response:\n",
    "    print(hit.meta.score, hit.title, hit.page_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HITS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bidict import bidict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = items\n",
    "# Give each author an id and store them in this dict\n",
    "author_ids = bidict()\n",
    "id_counter = 0\n",
    "for paper in papers:\n",
    "    paper_authors = paper[\"authors\"]\n",
    "    for author in paper_authors:\n",
    "        if author not in author_ids:\n",
    "            author_ids[author] = id_counter\n",
    "            id_counter += 1\n",
    "\n",
    "# Map each paper to it's authors' ids\n",
    "paper_authors_dict = dict()\n",
    "for paper in papers:\n",
    "    paper_id = paper[\"id\"]\n",
    "    paper_authors = paper[\"authors\"]\n",
    "    paper_authors_dict[paper_id] = []\n",
    "    for author in paper_authors:\n",
    "        author_id = author_ids[author]\n",
    "        paper_authors_dict[paper_id].append(author_id)\n",
    "\n",
    "# Map each author to the authors he/she has referenced\n",
    "author_references = dict()\n",
    "for paper in papers:\n",
    "    paper_authors = paper[\"authors\"]\n",
    "    paper_references = paper[\"references\"]\n",
    "    references = []\n",
    "    for reference_id in paper_references:\n",
    "        if reference_id in paper_authors_dict:\n",
    "            references += paper_authors_dict[reference_id]\n",
    "    for author in paper_authors:\n",
    "        author_id = author_ids[author]\n",
    "        if author_id not in author_references:\n",
    "            author_references[author_id] = []\n",
    "        author_references[author_id] += references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_authors = len(author_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "connectivity_matrix = np.zeros((num_authors, num_authors))\n",
    "for author, references in author_references.items():\n",
    "    for reference in references:\n",
    "        connectivity_matrix[author, reference] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ones(num_authors)\n",
    "h = np.ones(num_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    for i in range(num_authors):\n",
    "        h[i] = np.sum(connectivity_matrix[i, :] * a)\n",
    "    for i in range(num_authors):\n",
    "        a[i] = np.sum(connectivity_matrix[:, i].T * h)\n",
    "    a /= np.sum(a)\n",
    "    h /= np.sum(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_authors = []\n",
    "for i in np.argpartition(a, -4)[-4:]:\n",
    "    best_authors.append((author_ids.inverse[i], a[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Authors:\n",
      "Ilya Sutskever : 0.038691023767089466\n",
      "Geoffrey E. Hinton : 0.03239437930182725\n",
      "Alex Krizhevsky : 0.028018939726845844\n",
      "Yoshua Bengio : 0.02182020882571343\n"
     ]
    }
   ],
   "source": [
    "print(\"Best Authors:\")\n",
    "best_authors = sorted(best_authors, key=lambda x: -x[1])\n",
    "for author, authority in best_authors:\n",
    "    print(f\"{author} : {authority}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./MIR_Phase3/data/train.txt\", \"r\") as f:\n",
    "    train_contents = f.read()\n",
    "with open(\"./MIR_Phase3/data/vali.txt\", \"r\") as f:\n",
    "    val_contents = f.read()\n",
    "with open(\"./MIR_Phase3/data/test.txt\", \"r\") as f:\n",
    "    test_contents = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "\n",
    "@dataclass\n",
    "class QueryResult:\n",
    "    qid: int\n",
    "    doc_id: str\n",
    "    relevance: int\n",
    "    feature_vector: np.array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "def parse_data(data: str) -> Dict[int, List[QueryResult]]:\n",
    "    query_to_result_dict = dict()\n",
    "    for line in data.split('\\n'):\n",
    "        splitted_line = line.split()\n",
    "        try:\n",
    "            relevance = int(splitted_line[0])\n",
    "            qid = int(line.split()[1][4:])\n",
    "            doc_id = line.split()[50]\n",
    "            feature_vector = []\n",
    "            for feature in line.split()[2:48]:\n",
    "                feature_vector.append(float(feature.split(':')[1]))\n",
    "            if qid not in query_to_result_dict:\n",
    "                query_to_result_dict[qid] = []\n",
    "            query_to_result_dict[qid].append(QueryResult(qid, doc_id, relevance, np.array(feature_vector)))\n",
    "        except IndexError:\n",
    "            pass\n",
    "    return query_to_result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query_to_result_dict = parse_data(train_contents)\n",
    "val_query_to_result_dict = parse_data(val_contents)\n",
    "test_query_to_result_dict = parse_data(test_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "POSITIVE_CLASS = 1\n",
    "NEGATIVE_CLASS = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_model(query_to_result_dict: Dict[int, List[QueryResult]]):\n",
    "    items = []\n",
    "    for qid, results in query_to_result_dict.items():\n",
    "        # Sort in decresing order\n",
    "        results = sorted(results, key=lambda x: -x.relevance)\n",
    "        num_results = len(results)\n",
    "        for idx, result in enumerate(results):\n",
    "            for other_result in results[idx+1:-1]:\n",
    "                if result.relevance > other_result.relevance:\n",
    "                    positive_vector = result.feature_vector - other_result.feature_vector\n",
    "                    items.append((POSITIVE_CLASS, positive_vector))\n",
    "                    items.append((NEGATIVE_CLASS, -positive_vector))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data  = prepare_data_for_model(train_query_to_result_dict)\n",
    "val_data = prepare_data_for_model(val_query_to_result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
