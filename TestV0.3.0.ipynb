{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "from researchgate.researchgate.spiders.paper import PaperSpider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = CrawlerProcess(\n",
    "    settings={\n",
    "        \"FEEDS\": {\"items.json\": {\"format\": \"json\"}},\n",
    "        \"LOG_ENABLED\": False\n",
    "        #     \"LOG_LEVEL\": 'INFO',\n",
    "    }\n",
    ")\n",
    "process.crawl(PaperSpider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"items.json\") as f:\n",
    "    items = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '323694313',\n",
       " 'title': 'The Lottery Ticket Hypothesis: Training Pruned Neural Networks',\n",
       " 'abstract': 'Recent work on neural network pruning indicates that, at training time, neural networks need to be significantly larger in size than is necessary to represent the eventual functions that they learn. This paper articulates a new hypothesis to explain this phenomenon. This conjecture, which we term the \"lottery ticket hypothesis,\" proposes that successful training depends on lucky random initialization of a smaller subcomponent of the network. Larger networks have more of these \"lottery tickets,\" meaning they are more likely to luck out with a subcomponent initialized in a configuration amenable to successful optimization. This paper conducts a series of experiments with XOR and MNIST that support the lottery ticket hypothesis. In particular, we identify these fortuitously-initialized subcomponents by pruning low-magnitude weights from trained networks. We then demonstrate that these subcomponents can be successfully retrained in isolation so long as the subnetworks are given the same initializations as they had at the beginning of the training process. Initialized as such, these small networks reliably converge successfully, often faster than the original network at the same level of accuracy. However, when these subcomponents are randomly reinitialized or rearranged, they perform worse than the original network. In other words, large networks that train successfully contain small subnetworks with initializations conducive to optimization. The lottery ticket hypothesis and its connection to pruning are a step toward developing architectures, initializations, and training strategies that make it possible to solve the same problems with much smaller networks.',\n",
       " 'date': 2018,\n",
       " 'references': ['322058064',\n",
       "  '317100877',\n",
       "  '307536925',\n",
       "  '286794765',\n",
       "  '286761751',\n",
       "  '277959043',\n",
       "  '221618539',\n",
       "  '3568764',\n",
       "  '2985446'],\n",
       " 'authors': ['Jonathan Frankle', 'Michael Carbin']}"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "items[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticsearch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(hosts=[{\"host\": \"localhost\", \"port\": 9200}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clear previous index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(\"paper-index\", ignore=404)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'paper-index'}"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.create(index=\"paper-index\", ignore=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create persistent layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import (\n",
    "    Document,\n",
    "    Date,\n",
    "    Nested,\n",
    "    Boolean,\n",
    "    analyzer,\n",
    "    InnerDoc,\n",
    "    Completion,\n",
    "    Keyword,\n",
    "    Text,\n",
    "    Integer,\n",
    "    Float,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Paper(Document):\n",
    "    title = Text(fields={\"raw\": Keyword()})\n",
    "    date = Integer()\n",
    "    abstract = Text()\n",
    "    authors = Text()\n",
    "    references = Text()\n",
    "    page_rank = Float()\n",
    "\n",
    "    class Index:\n",
    "        name = \"paper-index\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the mappings in Elasticsearch\n",
    "Paper.init(using=es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert items "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in items:\n",
    "    paper = Paper(meta={\"id\": item[\"id\"]}, page_rank=1.0, **item)\n",
    "    paper.save(using=es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcions for inserting items and clearing index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_utils(host=None):\n",
    "    if host is None:\n",
    "        host = {\"host\": \"localhost\", \"port\": 9200}\n",
    "    es = Elasticsearch(hosts=[{\"host\": \"localhost\", \"port\": 9200}])\n",
    "\n",
    "    def clear_index():\n",
    "        es.indices.delete(\"paper-index\", ignore=404)\n",
    "        es.indices.create(index=\"paper-index\", ignore=400)\n",
    "\n",
    "    def insert_items(items):\n",
    "        for item in items:\n",
    "            paper = Paper(meta={\"id\": item[\"id\"]}, **item)\n",
    "            paper.save(using=es)\n",
    "\n",
    "    return clear_index, insert_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear, insert = gen_utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert(items)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating page rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = list(items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ids = sorted(list(map(lambda x: x[\"id\"], all_papers)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_matrix = np.zeros((len(all_ids), len(all_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_loc = dict()\n",
    "for index, paper_id in enumerate(all_ids):\n",
    "    id_loc[paper_id] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, paper_id in enumerate(all_ids):\n",
    "    paper = Paper.get(id=paper_id, using=es)\n",
    "    for reference_id in paper.references:\n",
    "        try:\n",
    "            p_matrix[index, id_loc[reference_id]] = 1\n",
    "        except KeyError:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "N = len(all_ids)\n",
    "v = np.ones((1, N))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_sums = np.sum(p_matrix, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first part is for rows having nonzero elements\n",
    "# second part is for dead-ends\n",
    "p_matrix = ((row_sums > 0) * 1) * (\n",
    "    (1 - alpha) * p_matrix / (row_sums + np.logical_not(row_sums > 0) * 1)\n",
    "    + alpha * v / N\n",
    ") + (np.logical_not(row_sums > 0) * 1) * v / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.ones((1, N)) / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    next_state = x0 @ p_matrix\n",
    "    if np.allclose(next_state, x0, rtol=0.0001):\n",
    "        break\n",
    "    x0 = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0281928 , 0.04087966, 0.0281928 , 0.1900929 , 0.04658897,\n",
       "        0.04087966, 0.08599568, 0.0277268 , 0.0281928 , 0.02562982,\n",
       "        0.0277268 , 0.0277268 , 0.07035748, 0.0281928 , 0.06474251,\n",
       "        0.02562982, 0.04020395, 0.02562982, 0.0277268 , 0.0277268 ,\n",
       "        0.0360449 , 0.0277268 , 0.0281928 ]])"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, paper_id in enumerate(all_ids):\n",
    "    paper = Paper.get(id=paper_id, using=es)\n",
    "    paper.update(page_rank=next_state[0, index], using=es)\n",
    "    paper.save(using=es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_search = \"lottery\"\n",
    "abstract_search = \"language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = es.search(\n",
    "    index=\"paper-index\",\n",
    "    body={\n",
    "        \"query\": {\n",
    "            # https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-function-score-query.html#function-weight\n",
    "            \"function_score\": {\n",
    "                \"functions\": [\n",
    "                    {\"filter\": {\"match\": {\"title\": \"lottery\"}}, \"weight\": 20},\n",
    "                    {\n",
    "                        \"filter\": {\"match\": {\"abstract\": \"language\"}},\n",
    "                        \"weight\": 40,\n",
    "                    },\n",
    "                    # https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-range-query.html\n",
    "                    {\n",
    "                        \"filter\": {\"range\": {\"date\": {\"gte\": 2018}}},\n",
    "                        \"weight\": 10,\n",
    "                    },\n",
    "                    # https://www.elastic.co/guide/en/elasticsearch/reference/current/static-scoring-signals.html\n",
    "                    # https://www.elastic.co/guide/en/elasticsearch/reference/7.x/query-dsl-rank-feature-query.html#rank-feature-query-saturation\n",
    "                    {\n",
    "                        \"script_score\": {\n",
    "                            \"script\": {\n",
    "                                \"source\": \"_score * saturation(doc['page_rank'].value, 10)\"\n",
    "                            }\n",
    "                        }\n",
    "                    },\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1060055\n",
      "1.0225718\n",
      "0.5112859\n",
      "0.34104985\n",
      "0.27946368\n",
      "0.25730416\n",
      "0.1854917\n",
      "0.16285291\n",
      "0.040042963\n",
      "0.035915446\n"
     ]
    }
   ],
   "source": [
    "for hit in res[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_index': 'paper-index',\n",
       " '_type': '_doc',\n",
       " '_id': '335685749',\n",
       " '_score': 1.1060055,\n",
       " '_source': {'page_rank': 0.027726804663415785,\n",
       "  'id': '335685749',\n",
       "  'title': 'Character-Level Language Modeling with Deeper Self-Attention',\n",
       "  'abstract': 'LSTMs and other RNN variants have shown strong performance on character-level language modeling. These models are typically trained using truncated backpropagation through time, and it is common to assume that their success stems from their ability to remember long-term contexts. In this paper, we show that a deep (64-layer) transformer model (Vaswani et al. 2017) with fixed context outperforms RNN variants by a large margin, achieving state of the art on two popular benchmarks: 1.13 bits per character on text8 and 1.06 on enwik8. To get good results at this depth, we show that it is important to add auxiliary losses, both at intermediate network layers and intermediate sequence positions.',\n",
       "  'date': 2019,\n",
       "  'references': ['334116459',\n",
       "   '325452116',\n",
       "   '323796618',\n",
       "   '319977201',\n",
       "   '319769995',\n",
       "   '318981690',\n",
       "   '317100692',\n",
       "   '315795238',\n",
       "   '313760677',\n",
       "   '312619873',\n",
       "   '311900760',\n",
       "   '311648230',\n",
       "   '311469848',\n",
       "   '308646548',\n",
       "   '305229133',\n",
       "   '303822179',\n",
       "   '301878902',\n",
       "   '301872874',\n",
       "   '301847993',\n",
       "   '287249598',\n",
       "   '284476210',\n",
       "   '281607724',\n",
       "   '272194743',\n",
       "   '267046290',\n",
       "   '266485700',\n",
       "   '266030628',\n",
       "   '265469170',\n",
       "   '262877889',\n",
       "   '259239818',\n",
       "   '13853244',\n",
       "   '2984354'],\n",
       "  'authors': ['Rami Al-Rfou', 'Dokook Choe', 'Noah Constant', 'Mandy Guo']}}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[\"hits\"][\"hits\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Search(using=es, index=\"paper-index\").query(\"match\", title=\"the lottery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = Search(using=es, index=\"paper-index\").query(\"match\", id=\"323694313\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3779216 The Lottery Ticket Hypothesis: Training Pruned Neural Networks\n"
     ]
    }
   ],
   "source": [
    "for hit in response:\n",
    "    print(hit.meta.score, hit.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by calling .search we get back a standard Search object\n",
    "s = Paper.search(using=es)\n",
    "# the search is already limited to the index and doc_type of our document\n",
    "s = s.query(\"match\", title=\"the lottery\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s.execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.6907296 The Lottery Ticket Hypothesis: Training Pruned Neural Networks 0.025629817277203964\n"
     ]
    }
   ],
   "source": [
    "for hit in response:\n",
    "    print(hit.meta.score, hit.title, hit.page_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
