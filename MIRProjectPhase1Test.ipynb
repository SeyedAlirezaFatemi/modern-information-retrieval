{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXCFFyc8Jnd7"
   },
   "source": [
    "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "<font color=red size=7>\n",
    "<p></p>\n",
    "<div align=center>توضیحات </div>\n",
    "</font>\n",
    "در این فایل قسمت هایی برای تست فاز اول پروژه قرار گرفته است. با اضافه شدن این قسمت ها به کد شما باید تمامی قسمت ها بدون اشکال اجرا شوند، در صورتی که شما فرضی به پروژه اضافه نکرده باشید و طبق استاندارد توضیح داده شده پیاده سازی کرده باشید، تمامی این قسمت ها بدون اشکال قابل اجرا هستند. اما در صورتی که قسمتی قابل اجرا نبود شما مجازید که با اعمال فرضیات خود کد تست را تغییر دهید تا تست ها به درستی اجرا شوند.\n",
    "پس از اعمال تغییرات مورد نیاز در همین فایل\n",
    "(بدون افزودن قسمت های قبلا پیاده سازی شده در پروژه)\n",
    "فایل را در سایت کوئرا آپلود کنید.  \n",
    "<strong>\n",
    "توجه کنید که تغییرات شما باید جزئی و صرفا در جهت ایجاد همخوانی در روند تست با فرضیات شما باشند، پس به هیچ عنوان نباید روند کلی تست ها را تغییر دهید! همچنین تمامی تغییرات و دلیل انجام آن ها را در همین فایل در سلول های مناسب شرح دهید.\n",
    "<br>\n",
    "همچنین هرگونه تغییر در تعداد ورودی ها و چیدمان آن ها غیر مجاز میباشد.\n",
    "</strong>\n",
    "</font></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNtjLD_zM11t"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=2>\n",
    "<h1 dir=\"rtl\">قسمت اول و دوم<h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models.TextPreparer import TextPreparer\n",
    "from src.utils.construct_positional_indexes import construct_positional_indexes\n",
    "from src.enums import *\n",
    "from src.utils.load_index import load_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1572/1572 [00:09<00:00, 163.73it/s]\n",
      "100%|██████████| 1572/1572 [00:18<00:00, 84.06it/s] \n"
     ]
    }
   ],
   "source": [
    "text_preparer = TextPreparer()\n",
    "manager = construct_positional_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code was previously defined in MIRProjectPhase1.ipynb; uploaded with the rest of my codes for the first phase\n",
    "\n",
    "\n",
    "def prepare_text(raw_text):\n",
    "    prepared_text = text_preparer.prepare_text(raw_text)\n",
    "    return prepared_text\n",
    "\n",
    "\n",
    "def get_posting_list(word):\n",
    "    posting_list = manager.corpus_index.get_posting_list(word)\n",
    "    results = dict()\n",
    "    for posting_list_item in posting_list:\n",
    "        result = dict()\n",
    "        result[\"text\"] = posting_list_item.get_positions(Fields.TEXT)\n",
    "        result[\"title\"] = posting_list_item.get_positions(Fields.TITLE)\n",
    "        results[posting_list_item.doc_id] = result\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_words_with_bigram(bigram):\n",
    "    return manager.bigram_index.get_words_with_bigram(bigram)\n",
    "\n",
    "\n",
    "def add_document_to_indexes(docs_path, doc_num):\n",
    "    manager.add_document_to_indexes(docs_path, doc_num)\n",
    "\n",
    "\n",
    "def delete_document_from_indexes(docs_path, doc_num):\n",
    "    manager.delete_document_from_indexes(docs_path, doc_num)\n",
    "\n",
    "\n",
    "def save_index(destination):\n",
    "    manager.save_index(destination)\n",
    "\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2):\n",
    "    return manager.search(\n",
    "        query,\n",
    "        method=Methods(method),\n",
    "        field_weights={Fields.TEXT: 1.0, Fields.TITLE: weight},\n",
    "        max_retrieved=15,\n",
    "        correct_word=True,\n",
    "    )\n",
    "\n",
    "\n",
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    return manager.search(\n",
    "        {Fields.TEXT: text_query, Fields.TITLE: title_query},\n",
    "        method=Methods(method),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KdO8v3QCFDf_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============ testing 'prepare_text' =============================================\n",
      "prepared text is : ['کتاب', 'مناسب', 'نوشته_شوند', 'در', 'راستا', 'ارتقا', 'سطح', 'آموز', 'کشور', 'تلاش', 'زیاد', 'صور', 'می\\u200cگیرد'] with length: 13\n",
      "\n",
      "============ testing 'get_posting_list' =========================================\n",
      "number of ocurrences of the word فکری  in documents =  168\n",
      "docs with the word: [3014, 3099, 3103, 3119, 3197, 3217, 3229, 3359, 3373, 3404, 3429, 3654, 3699, 3776, 3777, 3798, 3826, 3879, 3897, 3938, 4002, 4062, 4248, 4275, 4321, 4335, 4382, 4388, 4391, 4398, 4400, 4460, 4636, 4650, 4671, 4718, 4726, 4743, 4805, 4843, 4864, 5192, 5308, 5381, 5486, 5554, 5571, 5707, 5720, 5820, 5967, 6014, 6052, 6088, 6229, 6294, 6417, 6418, 6475, 6522, 6568, 6572, 6609, 6629, 6634, 6710, 6735, 6749, 6752, 6753, 6791, 6847, 6848, 6907, 6931, 6944, 6959, 6973, 7133]\n",
      "\n",
      "============ testing 'get_words_with_bigram' ====================================\n",
      "returned list length: 2403\n",
      "checking word هیلاندراس : True\n",
      "\n",
      "============ testing 'doc_remove' ================================================\n",
      "length of posting list for word فکری before removing doc 3014 : 79\n",
      "length of posting list for word فکری after removing doc 3014 : 78\n",
      "length of posting list for word هیلاندراس before removing doc 6752 : 1\n",
      "length of posting list for word هیلاندراس after removing doc 6752 : 0\n",
      "\n",
      "============ testing correct bigram removal ========================================\n",
      "returned list length: 2403\n",
      "checking word هیلاندراس : True\n",
      "\n",
      "============ testing 'doc_add' ================================================\n",
      "length of posting list for word فکری before adding doc 3014 : 77\n",
      "number of ocurrences for  فکری : 162\n",
      "length of posting list for word فکری after adding doc 3014 : 78\n",
      "number of ocurrences for  فکری : 167\n",
      "length of posting list for word هیلاندراس before adding doc 6752 : 0\n",
      "length of posting list for word هیلاندراس after adding doc 6752 : 1\n",
      "\n",
      "============ testing correct bigram removal ========================================\n",
      "returned list length: 2403\n",
      "checking word هیلاندراس : True\n"
     ]
    }
   ],
   "source": [
    "word1 = 'فکری'\n",
    "doc_id = 3014\n",
    "\n",
    "\n",
    "word2 = 'هیلاندراس'\n",
    "doc_id2 = 6752\n",
    "bigram = 'لا'\n",
    "\n",
    "def get_count (l):\n",
    "    i = [1 for _,t in l.items() for q in t['text']]\n",
    "    j = [1 for _,t in l.items() if 'title' in t.keys() for q in t['title']]\n",
    "    return len (i) + len(j)\n",
    "\n",
    "\n",
    "def test_prepare_text():\n",
    "    print (\"\\n============ testing 'prepare_text' =============================================\")\n",
    "    raw_text = \"کتابهای مناسبی نوشته شوند ! در راستای ارتقای . سطح آموزش کشور ؟ تلاش‌های زیادی صورت می‌گیرد\"\n",
    "    \n",
    "    prepared_text = text_preparer.prepare_text(raw_text)    \n",
    "\n",
    "    print(\"prepared text is :\", prepared_text , \"with length:\" , len (prepared_text))\n",
    "    \n",
    "test_prepare_text()\n",
    "\n",
    "def test_get_posting_list():\n",
    "    \n",
    "    print (\"\\n============ testing 'get_posting_list' =========================================\")\n",
    "    \n",
    "    prepared_text = prepare_text(word1)[0]\n",
    "    posting_list = get_posting_list(prepared_text)\n",
    "    # posting_list = {3014:{'title':[...] , 'text':[...]}}\n",
    "    \n",
    "    \n",
    "#     print (\"posting list for input\" , prepared_text, \"is :\", posting_list , \"with length:\" , len (posting_list))\n",
    "    print (\"number of ocurrences of the word\", word1 , \" in documents = \", get_count (posting_list))\n",
    "    print ('docs with the word:' , sorted (list (posting_list.keys())))\n",
    "    \n",
    "test_get_posting_list()\n",
    "\n",
    "\n",
    "def test_bigram():\n",
    "    print (\"\\n============ testing 'get_words_with_bigram' ====================================\")\n",
    "    \n",
    "    words_with_bigram = get_words_with_bigram(bigram)\n",
    "    print (\"returned list length:\" , len (words_with_bigram))\n",
    "\n",
    "    print (\"checking word\" , word2 , \":\", word2 in words_with_bigram)\n",
    "    \n",
    "\n",
    "check_bigram = True\n",
    "if check_bigram:\n",
    "    test_bigram()\n",
    "\n",
    "def test_doc_remove():\n",
    "    \n",
    "    print (\"\\n============ testing 'doc_remove' ================================================\")\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"before removing doc\" , doc_id, \":\" , len (posting_list))\n",
    "    \n",
    "    delete_document_from_indexes('data/Persian.xml', doc_id)\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"after removing doc\" , doc_id, \":\" , len (posting_list))\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "    print (\"length of posting list for word\" , word2 , \"before removing doc\" , doc_id2, \":\" , len (posting_list))\n",
    "    \n",
    "    delete_document_from_indexes('data/Persian.xml', doc_id2)\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "    print (\"length of posting list for word\" , word2 , \"after removing doc\" , doc_id2, \":\" , len (posting_list))\n",
    "    \n",
    "test_doc_remove()\n",
    "\n",
    "def test_doc_remove_bigram():\n",
    "    print (\"\\n============ testing correct bigram removal ========================================\")\n",
    "    words_with_bigram = get_words_with_bigram(bigram)\n",
    "    print (\"returned list length:\" , len (words_with_bigram))\n",
    "    print (\"checking word\" , word2 , \":\", word2 in words_with_bigram)\n",
    "    \n",
    "\n",
    "if check_bigram:\n",
    "    test_doc_remove_bigram()\n",
    "\n",
    "def test_doc_add():\n",
    "    print (\"\\n============ testing 'doc_add' ================================================\")\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"before adding doc\" , doc_id, \":\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "    add_document_to_indexes('data/Persian.xml', doc_id)\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"after adding doc\" , doc_id, \":\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "    print (\"length of posting list for word\" , word2 , \"before adding doc\" , doc_id2, \":\" , len (posting_list))\n",
    "    \n",
    "    add_document_to_indexes('data/Persian.xml', doc_id2)\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word2)[0])\n",
    "    print (\"length of posting list for word\" , word2 , \"after adding doc\" , doc_id2, \":\" , len (posting_list))\n",
    "    \n",
    "    \n",
    "test_doc_add()\n",
    "\n",
    "\n",
    "def test_doc_add_bigram():\n",
    "    \n",
    "    words_with_bigram = get_words_with_bigram(bigram)\n",
    "    print (\"returned list length:\" , len (words_with_bigram))\n",
    "    print (\"checking word\" , word2 , \":\", word2 in words_with_bigram)\n",
    "\n",
    "if check_bigram:\n",
    "    test_doc_remove_bigram()\n",
    "    \n",
    "def test_save_and_load ():\n",
    "    print (\"\\n============ testing save and load methods ========================================\")\n",
    "    \n",
    "    destination = \"storage/backup/manager.pickle\"\"\n",
    "\n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"before saving:\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "    save_index(destination)\n",
    "    manager = load_index(destination)\n",
    "    \n",
    "    posting_list = get_posting_list(prepare_text(word1)[0])\n",
    "    print (\"length of posting list for word\" , word1 , \"after loading:\" , len (posting_list))\n",
    "    print (\"number of ocurrences for \", word1, \":\", get_count (posting_list))\n",
    "    \n",
    "test_save_and_load ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNtjLD_zM11t"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=2>\n",
    "<h1 dir=\"rtl\">قسمت سوم<h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\">\n",
    "تابع زیر چون در فاز پیش مشخص نشده بود  تازه نوشته شده. در واقع این تابع در داخل کد سرچ وجود دارد.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_query(query: str):\n",
    "    query_tokens = text_preparer.prepare_text(query)\n",
    "    corrected_tokens = []\n",
    "    for word in query_tokens:\n",
    "        corrected_tokens.append(manager.correct_word(word))\n",
    "    return corrected_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGxtv7qZM90t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['اذار', 'فا', 'و', 'پیشرفته', 'نا']\n"
     ]
    }
   ],
   "source": [
    "def test_correct_query():\n",
    "\n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    query = \"ابذار های فظایی و پیشرفته ناصا\"\n",
    "    ##################################\n",
    "\n",
    "    result = correct_query(query)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "test_correct_query()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A9n-4JRDNDcS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5724, 3903, 4255, 3411, 3416, 3414, 6629, 3415, 6266, 4259, 4376, 4627, 4659, 4573, 4624]\n"
     ]
    }
   ],
   "source": [
    "def test_search():\n",
    "\n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    query = 'سیاره های بزرگ \"منظومه شمسی\"'\n",
    "    method = \"ltc-lnc\"\n",
    "    ##################################\n",
    "\n",
    "    relevant_docs = search(query, method)\n",
    "    print(relevant_docs)\n",
    "\n",
    "\n",
    "test_search()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi_cziuINH8x"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3760, 5264, 5236, 3758, 6369, 5644, 3874, 6394, 4275, 4094, 5381, 6159, 6417, 4339, 5932]\n"
     ]
    }
   ],
   "source": [
    "def test_detailed_search():\n",
    "\n",
    "    ##################################\n",
    "    ## Do not change this part\n",
    "    ##################################\n",
    "    title_query = \"فهرست شهرهای ایران\"\n",
    "    text_query = \"استان گیلان شهرستان لنگرود\"\n",
    "    ##################################\n",
    "\n",
    "    relevant_docs = detailed_search(title_query, text_query)\n",
    "    print(relevant_docs)\n",
    "\n",
    "\n",
    "test_detailed_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ePcovk3jG8aG"
   },
   "source": [
    "<div style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=2>\n",
    "<h1 dir=\"rtl\">قسمت چهارم<h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vidL0GkdHmBA"
   },
   "source": [
    "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در این قسمت به دلیل استفاده از utf8\n",
    "در فایل های داده، در بعضی از کد های ارسال شده پردازش فایل ها با خطا مواجه میشود برای حل این مشکل بدون دست بردن در کد های ارسال شده میتوانیم از سلول زیر استفاده کنیم.  \n",
    "درصورتی که شما نیازی به اعمال این تغییر ندارید میتوانید این سلول را در تست خود اجرا نکنید.\n",
    "<strong>\n",
    "در صورتی که نیاز به ایجاد تغییر در این سلول دارید حتما تغییرات خود را در یک سلول متنی شرح دهید!!!\n",
    "</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPyzE_xlFDgG"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    python_open\n",
    "    print(\"Already done!\")\n",
    "except NameError:\n",
    "    python_open = open\n",
    "\n",
    "    def open(\n",
    "        file,\n",
    "        mode=\"r\",\n",
    "        buffering=-1,\n",
    "        encoding=None,\n",
    "        errors=None,\n",
    "        newline=None,\n",
    "        closefd=True,\n",
    "        opener=None,\n",
    "    ):\n",
    "        encoding = \"utf-8\"\n",
    "        return python_open(\n",
    "            file,\n",
    "            mode=mode,\n",
    "            buffering=buffering,\n",
    "            encoding=encoding,\n",
    "            errors=errors,\n",
    "            newline=newline,\n",
    "            closefd=closefd,\n",
    "            opener=opener,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LOLBO7uMFDgK"
   },
   "outputs": [],
   "source": [
    "##################################\n",
    "## Do not change this part\n",
    "##################################\n",
    "test_docs = [\"all\", 1, 2, 3]\n",
    "# functions = {'R_Precision':R_Precision, 'F_measure':F_measure, 'MAP':MAP, 'NDCG': NDCG}\n",
    "##################################\n",
    "\n",
    "# for doc in test_docs:\n",
    "#     print(\"{}\\ndoc:\\t{}\".format('-'*30, doc))\n",
    "#     for f in functions.keys():\n",
    "#         out = functions[f](doc)\n",
    "#         print(\"{:11}:\\t{:.2f}\".format(f, out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "من قبلا این تابع را پیاده سازی کردم که دیگه همینجا استفاده می‌کنم\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.metrics import evaluate_search_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "doc:\tall\n",
      "r_precision: 0.6105804602529432\n",
      "ndcg: 0.6787020126128037\n",
      "f_measure: 0.6279770638289564\n",
      "map: 0.7569273771354816\n",
      "------------------------------\n",
      "doc:\t1\n",
      "r_precision: 0.6105804602529432\n",
      "ndcg: 0.6787020126128037\n",
      "f_measure: 0.6279770638289564\n",
      "map: 0.7569273771354816\n",
      "------------------------------\n",
      "doc:\t2\n",
      "r_precision: 0.6105804602529432\n",
      "ndcg: 0.6787020126128037\n",
      "f_measure: 0.6279770638289564\n",
      "map: 0.7569273771354816\n",
      "------------------------------\n",
      "doc:\t3\n",
      "r_precision: 0.6105804602529432\n",
      "ndcg: 0.6787020126128037\n",
      "f_measure: 0.6279770638289564\n",
      "map: 0.7569273771354816\n"
     ]
    }
   ],
   "source": [
    "for doc in test_docs:\n",
    "    print(\"{}\\ndoc:\\t{}\".format(\"-\" * 30, doc))\n",
    "    evaluate_search_engine(manager, method=Methods.LTN_LNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a3CaHWJKJVcH"
   },
   "source": [
    "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "نمونه خروجی:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "39oS5a9AFDgN"
   },
   "source": [
    "------------------------------\n",
    "    doc:\tall\n",
    "    R_Precision:\t0.73\n",
    "    F_measure  :\t0.65\n",
    "    MAP        :\t0.63\n",
    "    NDCG       :\t0.75\n",
    "    \n",
    "------------------------------\n",
    "    doc:\t1\n",
    "    R_Precision:\t1.00\n",
    "    F_measure  :\t0.97\n",
    "    MAP        :\t0.94\n",
    "    NDCG       :\t0.96\n",
    "    \n",
    "------------------------------\n",
    "    doc:\t2\n",
    "    R_Precision:\t1.00\n",
    "    F_measure  :\t0.91\n",
    "    MAP        :\t0.83\n",
    "    NDCG       :\t0.90\n",
    "    \n",
    "------------------------------\n",
    "    doc:\t3\n",
    "    R_Precision:\t0.73\n",
    "    F_measure  :\t0.49\n",
    "    MAP        :\t0.29\n",
    "    NDCG       :\t0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3vMlLNSJafk"
   },
   "source": [
    "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "در این قسمت توابع امتیاز دهی شما مستقل از سایر قسمت های پروژه تست میشوند. برای این کار توابع جستجو مجددا با ساختار ساده ای پیاده سازی شده اند تا خروجی ثابتی را تولید کنند.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from src.metrics import f_measure, ndcg_at_k, r_precision, average_precision\n",
    "from src.utils.read_queries import read_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_retrieved_relevance_and_num_relevant_docs(query_id):\n",
    "    queries, relevants = read_queries(query_id)\n",
    "    retrieved_docs = search(\"\")\n",
    "    retrieved_relevance = []\n",
    "    relevants = relevants[0]\n",
    "    for retrieved_doc in retrieved_docs:\n",
    "        if retrieved_doc in relevants:\n",
    "            retrieved_relevance.append(1)\n",
    "        else:\n",
    "            retrieved_relevance.append(0)\n",
    "    num_relevant_docs = len(relevants)\n",
    "    return retrieved_relevance, num_relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def R_Precision(query_id):\n",
    "    retrieved_relevance, num_relevant_docs = calc_retrieved_relevance_and_num_relevant_docs(\n",
    "        query_id\n",
    "    )\n",
    "    return r_precision(retrieved_relevance, num_relevant_docs)\n",
    "\n",
    "\n",
    "def F_measure(query_id):\n",
    "    retrieved_relevance, num_relevant_docs = calc_retrieved_relevance_and_num_relevant_docs(\n",
    "        query_id\n",
    "    )\n",
    "    return f_measure(retrieved_relevance, num_relevant_docs)\n",
    "\n",
    "\n",
    "def MAP(query_id):\n",
    "    retrieved_relevance, num_relevant_docs = calc_retrieved_relevance_and_num_relevant_docs(\n",
    "        query_id\n",
    "    )\n",
    "    return average_precision(retrieved_relevance)\n",
    "\n",
    "\n",
    "def NDCG(query_id):\n",
    "    retrieved_relevance, num_relevant_docs = calc_retrieved_relevance_and_num_relevant_docs(\n",
    "        query_id\n",
    "    )\n",
    "    return ndcg_at_k(\n",
    "        retrieved_relevance,\n",
    "        np.ones(len(retrieved_relevance)),\n",
    "        num_relevant_docs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rKPkT5jEFDgN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "R_Precision:\n",
      "1:\t1.00\tTrue\n",
      "2:\t0.44\tTrue\n",
      "3:\t0.00\tTrue\n",
      "------------------------------\n",
      "F_measure:\n",
      "1:\t1.00\tFalse\n",
      "2:\t0.62\tTrue\n",
      "3:\t0.00\tTrue\n",
      "------------------------------\n",
      "MAP:\n",
      "1:\t1.00\tFalse\n",
      "2:\t1.00\tFalse\n",
      "3:\t0.00\tTrue\n",
      "------------------------------\n",
      "NDCG:\n",
      "1:\t1.00\tFalse\n",
      "2:\t1.00\tFalse\n",
      "3:\t0.00\tTrue\n"
     ]
    }
   ],
   "source": [
    "##################################\n",
    "## Do not change this part\n",
    "##################################\n",
    "test_docs = [1, 2, 3]\n",
    "rels = [\n",
    "    [\n",
    "        6753,\n",
    "        7134,\n",
    "        6978,\n",
    "        7136,\n",
    "        4530,\n",
    "        6798,\n",
    "        6885,\n",
    "        5381,\n",
    "        6900,\n",
    "        4537,\n",
    "        5509,\n",
    "        6794,\n",
    "        4094,\n",
    "        6417,\n",
    "        3666,\n",
    "        5967,\n",
    "    ],\n",
    "    [6753, 5509, 4718, 6798, 6850, 6417, 6978, 6871],\n",
    "    list(range(20)),\n",
    "]\n",
    "outputs = [\n",
    "    {\n",
    "        \"R_Precision\": 1.0,\n",
    "        \"F_measure\": 0.967_741_935_483_871,\n",
    "        \"MAP\": 0.9375,\n",
    "        \"NDCG\": 0.963_564_011_026_350_9,\n",
    "    },\n",
    "    {\n",
    "        \"R_Precision\": 0.444_444_444_444_444_4,\n",
    "        \"F_measure\": 0.615_384_615_384_615_3,\n",
    "        \"MAP\": 0.444_444_444_444_444_4,\n",
    "        \"NDCG\": 0.631_380_202_279_965_8,\n",
    "    },\n",
    "    {\"R_Precision\": 0.0, \"F_measure\": 0.0, \"MAP\": 0.0, \"NDCG\": 0.0},\n",
    "]\n",
    "\n",
    "functions = {\n",
    "    \"R_Precision\": R_Precision,\n",
    "    \"F_measure\": F_measure,\n",
    "    \"MAP\": MAP,\n",
    "    \"NDCG\": NDCG,\n",
    "}\n",
    "##################################\n",
    "idx = 0\n",
    "\n",
    "ds = detailed_search\n",
    "s = search\n",
    "\n",
    "\n",
    "def detailed_search(title_query, text_query, method=\"ltn-lnn\"):\n",
    "    return rels[idx]\n",
    "\n",
    "\n",
    "def search(query, method=\"ltn-lnn\", weight=2):\n",
    "    return rels[idx]\n",
    "\n",
    "\n",
    "for f in functions.keys():\n",
    "    print(\"{}\\n{}:\".format(\"-\" * 30, f))\n",
    "    idx = 0\n",
    "    for doc in test_docs:\n",
    "        out = functions[f](doc)\n",
    "        expected = outputs[idx][f]\n",
    "        print(\"{}:\\t{:.2f}\\t{}\".format(doc, out, abs(out - expected) < 1e-3))\n",
    "        idx += 1\n",
    "\n",
    "detailed_search = ds\n",
    "search = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "فکر کنم شاید در تعریف تابع های ارزیابی یکم تفاوت وجود داره اما من تا جایی که می‌دونم توابع را درست پیاده سازی کردم.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JVFa3UbdJ_5W"
   },
   "source": [
    "<div dir=\"rtl\" style=\"direction:rtl;line-height:300%;text-align:justify;\" align=\"justify\"><font face=\"XB Zar\" size=5>\n",
    "نمونه خروجی:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m8vPRt7GFDgS"
   },
   "source": [
    "------------------------------\n",
    "    R_Precision: \n",
    "    1:\t1.00\tTrue \n",
    "    2:\t0.44\tTrue \n",
    "    3:\t0.00\tTrue \n",
    "    \n",
    "------------------------------\n",
    "    F_measure: \n",
    "    1:\t0.97\tTrue\n",
    "    2:\t0.62\tTrue\n",
    "    3:\t0.00\tTrue\n",
    "\n",
    "------------------------------\n",
    "    MAP: \n",
    "    1:\t0.94\tTrue\n",
    "    2:\t0.44\tTrue\n",
    "    3:\t0.00\tTrue\n",
    "\n",
    "------------------------------\n",
    "    NDCG:\n",
    "    1:\t0.96\tTrue\n",
    "    2:\t0.63\tTrue\n",
    "    3:\t0.00\tTrue\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "MIRProjectPhase1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
